"""
SFT í•™ìŠµ ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (ë¡œì»¬ ëª¨ë¸ ë²„ì „)
ë¡œì»¬ HuggingFace ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìˆ˜í•™ ìˆ˜ëŠ¥ ë¬¸ì œì— ëŒ€í•œ í’€ì´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python generate_sft_data_local.py --model_path /path/to/gpt-oss-12b [ì˜µì…˜]
"""
import os
import time
import glob
import torch
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForCausalLM

# generate_sft_data.pyì—ì„œ ê³µí†µ í•¨ìˆ˜ ì„í¬íŠ¸
from generate_sft_data import (
    open_jsonl,
    to_jsonl,
    is_multiple_choice,
    extract_choice_value,
    clean_problem_text,
    get_prompt,
    format_output,
    merge_results,
    find_math_files,
)

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                    ğŸ”§ ê¸°ë³¸ ì„¤ì •                                          â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DEFAULT_DATA_DIR = "./data"
DEFAULT_OUTPUT_DIR = "./sft_output_local"
DEFAULT_MODEL_PATH = "/data/hf_models/gpt-oss-12b"
DEFAULT_N = 1
DEFAULT_FORMAT = "sharegpt"


# ============================================================================
# ë¡œì»¬ ëª¨ë¸ ì¶”ë¡ 
# ============================================================================

def load_local_model(model_path: str):
    """ë¡œì»¬ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    print(f"[INFO] Loading model from: {model_path}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path, add_eos_token=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype="auto",
        device_map="auto"
    )
    model.eval()
    
    print(f"[INFO] Model loaded successfully. Device: {next(model.parameters()).device}")
    return model, tokenizer


def generate_with_local_model(model, tokenizer, prompt: str, reasoning_effort: str = "high",
                               max_new_tokens: int = 10000, temperature: float = 1.0) -> str:
    """ë¡œì»¬ ëª¨ë¸ë¡œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    messages = [{"role": "user", "content": prompt}]
    
    chat_kwargs = {}
    if reasoning_effort != "none":
        chat_kwargs["reasoning_effort"] = reasoning_effort
    
    inputs = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        return_tensors="pt",
        return_dict=True,
        **chat_kwargs
    ).to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=True if temperature > 0 else False
        )
    
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=False)
    return decoded


def parse_gpt_oss_output(decoded_output: str) -> str:
    """GPT-OSS ì¶œë ¥ì—ì„œ assistant ì‘ë‹µì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    # final<|message|> íƒœê·¸ ì´í›„ê°€ ìµœì¢… ì‘ë‹µ
    if 'final<|message|>' in decoded_output:
        parts = decoded_output.split('final<|message|>')
        if len(parts) >= 2:
            solution = parts[-1].strip()
            # reasoning ë¶€ë¶„ ì¶”ì¶œ (ìˆëŠ” ê²½ìš°)
            if '<|reasoning|>' in decoded_output:
                reasoning_parts = decoded_output.split('<|reasoning|>')
                if len(reasoning_parts) >= 2:
                    reasoning = reasoning_parts[-1].split('final<|message|>')[0].strip()
                    solution = f"<think>\n{reasoning}\n</think>\n{solution}"
            return solution
    
    # ì¼ë°˜ì ì¸ assistant ì‘ë‹µ ì¶”ì¶œ
    if '<|assistant|>' in decoded_output:
        parts = decoded_output.split('<|assistant|>')
        if len(parts) >= 2:
            return parts[-1].strip()
    
    return decoded_output


# ============================================================================
# í•­ëª© ì²˜ë¦¬
# ============================================================================

def process_item_local(idx: tuple, problems: list, request_sentences: list, 
                       output_dir: str, model, tokenizer, source: str, 
                       format_type: str, question_type: str = "multiples",
                       reasoning_effort: str = "high"):
    """ë‹¨ì¼ ë¬¸ì œ-ìƒì„± ìŒì„ ë¡œì»¬ ëª¨ë¸ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    problem_idx, gen_idx = idx
    output_path = f"{output_dir}/{problem_idx}_{gen_idx}.jsonl"
    
    if os.path.exists(output_path):
        return None
    
    req_start = time.time()
    start_stamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"SEND [{problem_idx}_{gen_idx}] ({question_type}) | st={start_stamp}")
    
    item = problems[problem_idx]
    as_subjective = (question_type == "subjectives")
    prompt = get_prompt(item['problem'], request_sentences, gen_idx, as_subjective=as_subjective)
    
    try:
        decoded = generate_with_local_model(model, tokenizer, prompt, reasoning_effort)
        solution = parse_gpt_oss_output(decoded)
    except Exception as e:
        print(f"[{problem_idx}_{gen_idx}] EXCEPTION: {e}")
        return None
    
    answer = item.get('answer', None)
    
    if as_subjective and is_multiple_choice(item['problem']) and answer is not None:
        real_answer = extract_choice_value(item['problem'], answer)
    else:
        real_answer = answer
    
    formatted = format_output(
        problem=item['problem'],
        solution=solution,
        answer=real_answer,
        source=source,
        generation_id=gen_idx,
        format_type=format_type,
        prompt=prompt
    )
    
    to_jsonl(output_path, [formatted])
    
    req_duration = time.time() - req_start
    print(f"DONE [{problem_idx}_{gen_idx}] | time={req_duration:.2f}s")
    return formatted


def run_generation_local(problems: list, request_sentences: list, output_dir: str,
                          model, tokenizer, source: str, format_type: str,
                          n: int = 1, question_type: str = "multiples",
                          reasoning_effort: str = "high"):
    """ë¡œì»¬ ëª¨ë¸ë¡œ ëª¨ë“  ë¬¸ì œì— ëŒ€í•´ në²ˆì”© í’€ì´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    inputs = [(i, j) for i in range(len(problems)) for j in range(n)]
    print(f"Total tasks: {len(inputs)} ({len(problems)} problems x {n} generations) [{question_type}]")
    
    for idx in inputs:
        process_item_local(
            idx, problems, request_sentences,
            output_dir, model, tokenizer, source, format_type, question_type,
            reasoning_effort
        )


# ============================================================================
# ë©”ì¸
# ============================================================================

def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="SFT í•™ìŠµ ë°ì´í„° ìƒì„±ê¸° (ë¡œì»¬ ëª¨ë¸ ë²„ì „)",
        epilog="ì˜ˆì‹œ: python generate_sft_data_local.py --model_path /path/to/gpt-oss-12b"
    )
    
    parser.add_argument("--data_dir", default=DEFAULT_DATA_DIR, type=str)
    parser.add_argument("--output_dir", default=DEFAULT_OUTPUT_DIR, type=str)
    parser.add_argument("--input_file", type=str, default=None)
    parser.add_argument("--model_path", default=DEFAULT_MODEL_PATH, type=str)
    parser.add_argument("--reasoning_effort", default="high", type=str,
                        choices=["none", "low", "medium", "high"])
    parser.add_argument("--n", default=DEFAULT_N, type=int)
    parser.add_argument("--format", default=DEFAULT_FORMAT, type=str,
                        choices=["simple", "sharegpt", "alpaca"])
    parser.add_argument("--instruction_file", type=str, default="sentences_ask_boxed_kr.jsonl")
    parser.add_argument("--merge_only", action="store_true")
    parser.add_argument("--subjectives_only", action="store_true")
    parser.add_argument("--multiples_only", action="store_true")
    
    args = parser.parse_args()
    
    # instruction ë¬¸ì¥ ë¡œë“œ
    sentences_path = os.path.join(args.data_dir, args.instruction_file)
    if not os.path.exists(sentences_path):
        raise FileNotFoundError(f"{args.instruction_file} not found: {sentences_path}")
    request_sentences = open_jsonl(sentences_path)
    print(f"Loaded instruction file: {args.instruction_file}")
    
    # ì²˜ë¦¬í•  ìˆ˜í•™ íŒŒì¼ ëª©ë¡
    math_files = [args.input_file] if args.input_file else find_math_files(args.data_dir)
    if not math_files:
        raise FileNotFoundError(f"No math JSONL files found in {args.data_dir}")
    
    print(f"Found {len(math_files)} math files: {[os.path.basename(f) for f in math_files]}")
    
    result_dirs = []
    
    if not args.merge_only:
        model, tokenizer = load_local_model(args.model_path)
        
        for file_path in math_files:
            file_name = os.path.basename(file_path)
            source = file_name.replace('.jsonl', '')
            
            print(f"\n{'='*60}\nProcessing: {file_name}\n{'='*60}")
            
            problems = open_jsonl(file_path)
            print(f"Loaded {len(problems)} problems from {file_name}")
            
            mc_count = sum(1 for p in problems if is_multiple_choice(p['problem']))
            print(f"  - ê°ê´€ì‹: {mc_count}ê°œ, ì£¼ê´€ì‹: {len(problems) - mc_count}ê°œ")
            
            subj_output_dir = os.path.join(args.output_dir, source, "subjectives")
            mc_output_dir = os.path.join(args.output_dir, source, "multiples")
            os.makedirs(subj_output_dir, exist_ok=True)
            os.makedirs(mc_output_dir, exist_ok=True)
            result_dirs.extend([subj_output_dir, mc_output_dir])
            
            if not args.multiples_only:
                print(f"\n[ì£¼ê´€ì‹ ë²„ì „ ìƒì„±] ({len(problems)}ê°œ ë¬¸ì œ)")
                run_generation_local(problems, request_sentences, subj_output_dir,
                                      model, tokenizer, source, args.format, args.n,
                                      "subjectives", args.reasoning_effort)
            
            if not args.subjectives_only:
                print(f"\n[ê°ê´€ì‹ ë²„ì „ ìƒì„±] ({len(problems)}ê°œ ë¬¸ì œ)")
                run_generation_local(problems, request_sentences, mc_output_dir,
                                      model, tokenizer, source, args.format, args.n,
                                      "multiples", args.reasoning_effort)
    else:
        for file_path in math_files:
            source = os.path.basename(file_path).replace('.jsonl', '')
            for qtype in ["multiples", "subjectives"]:
                each_output_dir = os.path.join(args.output_dir, source, qtype)
                if os.path.exists(each_output_dir):
                    result_dirs.append(each_output_dir)
    
    # ê²°ê³¼ ë³‘í•©
    print(f"\n{'='*60}\nMerging results...\n{'='*60}")
    merged_path = os.path.join(args.output_dir, "merged", f"sft_math_all_{args.format}.jsonl")
    merge_results(result_dirs, merged_path)
    
    print("\nDONE!")


if __name__ == "__main__":
    main()
